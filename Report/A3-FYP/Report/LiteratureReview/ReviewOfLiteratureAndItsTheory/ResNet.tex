\subsubsection{ResNet}
Source: \parencite{targ2016resnet}
\newline
Article title: Resnet in Resnet: Generalizing Residual Architectures
\newline
Tags: [ResNet], [CNN]
\newline
Description:
\newline
This article is explaining how ResNet can be used within itself. This could help us improve the performance of ResNet since based on the article there is no computational overhead.
\newline

Source: \parencite{li2016demystifying}
\newline
Article title: Demystifying ResNet 
\newline
Tags: [ResNet], [NN] Description:
\newline
This article is deeply connected with our project since it goes into the depths of ResNet. Due to ResNet being a candidate algorithm, this article can give us more information if the choice is correct or not.
\newline

Source:\parencite{szegedy2017inception}
\newline
Article title: Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning
\newline
Tags: [CNN], [ResNet]
\newline
Description:
\newline
Inception networks are example of “very deep convolutional networks”. Due to that, they have been in the centre of image recognition over the years. This article would explain to us how inception networks with combination of ResNet can accelerate the speed and efficiency.
\newline

Source: \parencite{chen2017resnet}
\newline
Article title: ResNet and Model Fusion for Automatic Spoofing Detection 
\newline
Tags: [ResNet], [CNN]
\newline
Description:
\newline
In this article, they are using ResNet to detect if speakers have been spoofed or not. The article is not related to our topic, but it is an interesting view of ResNet in the sound industry and how it can be applied on different systems.
\newline

Source: \parencite{akiba2017extremely}
\newline
Article title: Extremely Large Minibatch SGD: Training ResNet-50 on ImageNet in 15 Minutes 
\newline
Tags: [ResNet]
\newline
Description:
\newline
This is one of the key articles for ResNet. The algorithm (ResNet) is being praised for its efficiency, therefore with this study we can see why. They are testing and proving how efficient ResNet50 is and based on that we can get some information if this is appropriate for our use.
\newline

Source: \parencite{lin2018resnet}
\newline
Article title: ResNet with one-neuron hidden layers is a Universal Approximator 
\newline
Tags: [ResNet], [NN]
\newline
Description:
\newline
This article is explaining how increasing representational power for narrow deep networks is due to one-neuron hidden layers. The article itself does not corelate well with our theme, but in case ResNet is chosen as primary algorithm, this might be something worth looking into.
\newline

Source: \parencite{allen2019can}
\newline
Article title: What Can ResNet Learn Efficiently, Going Beyond Kernels?
\newline
Tags: [ResNet], [NN]
\newline
Description:
\newline
This article is talking about efficiency of ResNet. The algorithm itself is known to be efficient, but this article is proving that with questioning how it can get accuracy of more than 96percent in CIFAR-10. The article is a great opportunity for us to evaluate performance of ResNet.
\newline

Source: \parencite{farooq2020covid}
\newline
Article title: COVID-ResNet: A Deep Learning Framework for Screening of COVID19 from Radiographs
\newline
Tags: [ResNet], [CNN]
\newline
Description:
\newline
This article is in the similar zone in terms of topic compared to ours. The researchers are scanning Covid19 x-ray images and determining result with ResNet. Due to this, we could get a sense of ResNet, its limits and if it is good for our application.
\newline