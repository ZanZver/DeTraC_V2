\subsection{Review}
In previous section(s) we have gone across themes and technologies that are going to help us. This section is describing how technologies have evolved over time and who the most important actors are.
\newline

VGG16 originated at Oxford university under the name of Visual Geometry Group (VGG) in the year 1989 \parencite{zisserman1989information}. The authors did publish the research at first as a mathematical paper, but nowadays it is used in the computer vison \parencite{simonyan2014very}. In the 1991, VGG team used the mathematics for the first time for the 3D object recognition \parencite{forsyth1991invariant}. Now days, team is working on different parts of image recognition and VGG network is still being developed, now moving into VGG19 \parencite{xiao2020application}.
\newline

Arguably roots of ResNet originated in 1996 \parencite{fitzpatrick1996functional} when Fitzpatrick D. did research on Functional Organization of Local Circuits. Thomson A. \parencite{thomson2010neocortical} has published another research in 2010 that prompted the research of ResNet. But why did nobody do any work prior to that? The reason is that ResNet is neural network that has similar structure to residential nets in human body, cortical layer VI neurons \parencite{proulx2014nicotinic} is a bit more specific example of it. In 2015, paper was published with the title of “Highway Networks” \parencite{srivastava2015highway}. It describes what ResNet shall become. In 2016, ResNet is created in the paper “Deep Residual Learning for Image Recognition” \parencite{he2016deep}. To this day, ResNet is developing itself as ResNet, ResNext, Wide ResNext... it all depends on the task that is needed on.
\newline

In the 2017, Facebook (with the help of University San Diego) has made a publicly available code of ResNext \parencite{web:resNeXtGitHub}. A research paper has also been published on that theme \parencite{xie2017aggregated}. Paper is describing how they have changed ResNet to ResNext with adding some features (example: new dimension “cardinality”). In the paper, they have compared ResNet101 and ResNext101 and seen that ResNext is an improvement over ResNet, especially in classification accuracy.
\newline

Transfer learning started its development in 1976 by Stevo B \parencite{bozinovski2020reminder}. The paper gives the mathematical and geometrical model on how transfer learning could work with neural networks. In 1993, first paper was published with machine learning in the combination of transfer learning \parencite{pratt1992discriminability}. Since then, the field has advanced to include multitask learning \parencite{thrun1998learning}. Now, transfer learning is becoming more and more popular in machine learning tasks, Andrew Ng has predicted that this is going to happen in 2016 \parencite{ng2016nuts}.
\newline

Neural network development started in 1943 with algorithms and threshold logic \parencite{mcculloch1943logical}. In 1940 mechanise of neural plasticity or Hebbian learning was discovered \parencite{hebb1940human}. All the information combined, helped with first computers at the time as well as “primitive neural networks”. Werbos’s research in 1975 \parencite{werbos1975experimental} reignited the interest in neural networks. The research paper showed the practical training in multi-layer networks. In 1986, neural networks with parallel distributed processing became popular due to the article that was published on that theme \parencite{rumelhart1986general}. The article explained how neurons can be simulated with the help of connectionism. In the year of 1992, multi-level networks were proposed with pretrained one layer and using unsupervised learning \parencite{schmidhuber1992learning}. In 2006 it was Hilton G. that proposed used of RBM with the high-level representation and real valued variables \parencite{hinton2006reducing}.
\newline

Convolutional neural networks (CNN) use mathematical operation called convolution for its work. They are mostly used in image processing now days. The work on CNNs started in 1959 by analysing cats visual cortices \parencite{hubel1959receptive}. This work was then upgrade in 1980 by Fukushima \parencite{fukushima1982neocognitron}. The newly discovered work (neocognitron) introduced two new layers: convolutional layers, and downsampling layers. In 1989, they have used CNN to read handwritten ZIP code numbers \parencite{lecun1989backpropagation}. Besides CNNs being discovered early on, they needed a lot of computational power to run. Due to that, CNNs started becoming more and more popular in 2000s since computing power is increasing every year \parencite{oh2004gpu}. Now days, CNNs are implemented in a lot of NN technologies due to their efficiency.
\newline

In the book questioning who started with deep learning \parencite{tappert2019father}, conclusions are drown towards Rosenblatt F in his book \parencite{rosenblatt1961principles}. in 1961. First working example of deep learning was published in 1967 \parencite{ivakhnenko1967cybernetics}. The term “deep learning” was firstly introduced in 1986 \parencite{dechter1988network}. Due to computational power, deep learning was not developing so fast compared to neural network development, but other fields in the AI have overcome some problems of it beforehand. After the computational power became cheaper (2000s), CNNs have evolved and that allowed deep learning to evolve as well.
\newline

Clustering (or k-means clustering) is a method of vector quantization. The terms “k-means” was firstly used in 1967 by James M \parencite{huber1967proceedings}. The algorithm was first proposed in 1957, but article was published later in 1982 \parencite{lloyd1982least}. Now days, clustering is used in AI all the time.
